{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 共通ライブラリー"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.pylab as pylab\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.linear_model import Perceptron\nfrom sklearn import svm \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.model_selection import cross_val_predict \nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **モデル１**"},{"metadata":{},"cell_type":"markdown","source":"# データの読み込み\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# 読み込むデータが格納されたディレクトリのパス，必要に応じて変更の必要あり\n\npath = \"/kaggle/input/titanic/\"\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ncombine = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 特徴量エンジニアリング"},{"metadata":{"trusted":true},"cell_type":"code","source":"combine[\"Sex\"] = combine[\"Sex\"].replace(\"male\", \"0\").replace(\"female\", \"1\")\ncombine[\"Sex\"] = combine[\"Sex\"].astype(int)\n\ncombine[\"Age\"].fillna(combine.Age.mean(), inplace=True) \n\ncombine['honorific'] = combine['Name'].map(lambda x: x.split(', ')[1].split('. ')[0])\ncombine['honorific'] = combine['honorific'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ncombine['honorific'] = combine['honorific'].replace('Mlle', 'Miss')\ncombine['honorific'] = combine['honorific'].replace('Ms', 'Miss')\ncombine['honorific'] = combine['honorific'].replace('Mme', 'Mrs')\nSalutation_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} \ncombine['honorific'] = combine['honorific'].map(Salutation_mapping) \ncombine['honorific'] = combine['honorific'].fillna(0) \n\ncombine[\"FamilySize\"] = combine[\"SibSp\"] + combine[\"Parch\"] + 1\n\ncombine['IsAlone'] = 0\ncombine.loc[combine['FamilySize'] == 1, 'IsAlone'] = 1\n\ncombine['Ticket_Alphabet'] = combine['Ticket'].apply(lambda x: str(x)[0])\ncombine['Ticket_Alphabet'] = combine['Ticket_Alphabet'].apply(lambda x: str(x)) \ncombine['Ticket_Alphabet'] = np.where((combine['Ticket_Alphabet']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), combine['Ticket_Alphabet'], np.where((combine['Ticket_Alphabet']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0')) \ncombine['Ticket_Alphabet']=combine['Ticket_Alphabet'].replace(\"1\",1).replace(\"2\",2).replace(\"3\",3).replace(\"0\",0).replace(\"S\",3).replace(\"P\",0).replace(\"C\",3).replace(\"A\",3) \ncombine['Ticket_Len'] = combine['Ticket'].apply(lambda x: len(x)) \n    \ncombine['Cabin_Alphabet'] = combine['Cabin'].apply(lambda x: str(x)[0]) \ncombine['Cabin_Alphabet'] = combine['Cabin_Alphabet'].apply(lambda x: str(x)) \ncombine['Cabin_Alphabet'] = np.where((combine['Cabin_Alphabet']).isin([ 'F', 'E', 'D', 'C', 'B', 'A']),combine['Cabin_Alphabet'], np.where((combine['Cabin_Alphabet']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0'))\ncombine['Cabin_Alphabet']=combine['Cabin_Alphabet'].replace(\"A\",1).replace(\"B\",2).replace(\"C\",1).replace(\"0\",0).replace(\"D\",2).replace(\"E\",2).replace(\"F\",1) \n\ncombine['Fare'].fillna(combine['Fare'].median(), inplace = True)\n\ncombine['Embarked'] = combine['Embarked'].fillna(combine[\"Embarked\"].mode()[0])\ncombine['Embarked'] = combine['Embarked'].replace('S', \"0\").replace( 'C', \"1\").replace('Q', \"2\")\ncombine['Embarked'] = combine['Embarked'].astype(int)\ncombine[\"Embarked\"].fillna(combine.Embarked.mean(), inplace=True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = combine[combine[\"train\"]==1]\ntest = combine[combine[\"train\"]==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"train\"], axis=1, inplace=True)\ntest.drop(['PassengerId', \"Name\", \"Ticket\", \"Cabin\", \"train\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train  = train[\"Survived\"]  \nX_train = train.drop([\"Survived\"], axis=1)\nX_test = test.drop([\"Survived\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ベストモデルリング１（RandomForestClassifier）"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=25, max_features='auto', max_leaf_nodes=None,\n            min_samples_leaf=1, min_samples_split=15,\n            min_weight_fraction_leaf=0.0, n_estimators=51, n_jobs=4,\n            oob_score=False, random_state=0, verbose=0, warm_start=False)\n\nrandom_forest.fit(X_train, y_train)\nY_pred_rf = random_forest.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 後処理"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/titanic/\"\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\n\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ndata = pd.concat([train, test])\ndata[\"Name\"] = data[\"Name\"].apply(lambda x: str(x)[x.find(\",\")+2 : x.find(\".\")])\ndata[\"Female_boy\"] = 0\ndata.loc[(data[\"Sex\"]==\"female\")|(data[\"Name\"]==\"Master\"), \"Female_boy\"] = 1\n\ndata = data[[\"PassengerId\", 'Survived', 'Ticket', 'Female_boy', \"train\"]]\ndata = data[data['Female_boy']==1]\nindex = data.index\nnum_Ticket = data.Ticket.value_counts()\nnum_Ticket = pd.DataFrame(num_Ticket)\nnum_Ticket.reset_index(inplace=True)\nnum_Ticket.rename({'index': 'Ticket', 'Ticket':'num_Ticket'}, inplace=True, axis=1)\ndata = pd.merge(data, num_Ticket, on=['Ticket'], how='left')\ndata['Ticket_sv'] = data.groupby('Ticket')['Survived'].transform('mean')\ndata.index = index\ntest_data = data[data[\"train\"]==0]\ntest_data = test_data[[\"PassengerId\", 'Ticket_sv']]\n\nsubmission_rf = pd.read_csv(path + \"gender_submission.csv\")\nsubmission_rf[\"Survived\"] = Y_pred_rf\nsubmission_rf = submission_rf.merge(test_data, on=[\"PassengerId\"], how=\"left\")\nsubmission_rf.loc[submission_rf['Ticket_sv']==1.0, \"Survived\"] = 1\nsubmission_rf.loc[submission_rf['Ticket_sv']==0.0, \"Survived\"] = 0\ndel submission_rf['Ticket_sv']\nsubmission_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル２"},{"metadata":{},"cell_type":"markdown","source":"# データの読み込み"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ncombine = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine[\"Sex\"] = combine[\"Sex\"].replace(\"male\", \"0\").replace(\"female\", \"1\")\ncombine[\"Sex\"] = combine[\"Sex\"].astype(int)\n\ncombine['honorific'] = combine['Name']\nfor name_string in combine['Name']:\n    combine['honorific'] = combine['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss','Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ncombine.replace({'honorific': mapping}, inplace=True)\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    age_to_impute = combine.groupby('honorific')['Age'].median()[titles.index(title)]\n    combine.loc[(combine['Age'].isnull()) & (combine['honorific'] == title), 'Age'] = age_to_impute\ncombine.drop('honorific', axis = 1, inplace = True)\n\ncombine['Family_Size'] = combine['Parch'] + combine['SibSp']\ncombine['Family_Name'] = combine['Name'].apply(lambda x: str.split(x, \",\")[0])\n\npre = 0.5\ncombine['Family_Survival'] = pre\nfor grp, grp_df in combine[['Survived','Name', 'Family_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Family_Name', 'Fare']):\n    if (len(grp_df) != 1):\n        # len(grp_df) != 1なら家族がいるって事\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 0\n                \nfor _, grp_df in combine.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    combine.loc[combine['PassengerId'] == passID, 'Family_Survival'] = 0\n\n\nlabel = LabelEncoder()\ncombine['Fare'].fillna(combine['Fare'].median(), inplace = True)\ncombine['Farelabal'] = pd.qcut(combine['Fare'], 5)\ncombine['Farelabal'] = label.fit_transform(combine['Farelabal'])\n\ncombine['Agelabel'] = pd.qcut(combine['Age'], 4)\ncombine['Agelabel'] = label.fit_transform(combine['Agelabel'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = combine[combine[\"train\"]==1]\ntest = combine[combine[\"train\"]==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'train', 'Family_Name'], axis = 1, inplace = True)\ntest.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin','Embarked', 'train', 'Family_Name'], axis = 1, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('Survived',  axis = 1)\ny = train['Survived']\nX_test = test.drop('Survived',  axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_test = std_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ベストモデリング２（KNeighborsClassifier）"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n                           weights='uniform')\nknn.fit(X, y)\ny_pred_knn = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 後処理"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/titanic/\"\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\n\ntrain[\"train\"] = 1\ntest[\"train\"] = 0\ndata = pd.concat([train, test])\ndata[\"Name\"] = data[\"Name\"].apply(lambda x: str(x)[x.find(\",\")+2 : x.find(\".\")])\ndata[\"Female_boy\"] = 0\ndata.loc[(data[\"Sex\"]==\"female\")|(data[\"Name\"]==\"Master\"), \"Female_boy\"] = 1\n\ndata = data[[\"PassengerId\", 'Survived', 'Ticket', 'Female_boy', \"train\"]]\ndata = data[data['Female_boy']==1]\nindex = data.index\nnum_Ticket = data.Ticket.value_counts()\nnum_Ticket = pd.DataFrame(num_Ticket)\nnum_Ticket.reset_index(inplace=True)\nnum_Ticket.rename({'index': 'Ticket', 'Ticket':'num_Ticket'}, inplace=True, axis=1)\ndata = pd.merge(data, num_Ticket, on=['Ticket'], how='left')\ndata['Ticket_sv'] = data.groupby('Ticket')['Survived'].transform('mean')\ndata.index = index\ntest_data = data[data[\"train\"]==0]\ntest_data = test_data[[\"PassengerId\", 'Ticket_sv']]\n\nsubmission_knn = pd.read_csv(path + \"gender_submission.csv\")\nsubmission_knn[\"Survived\"] = y_pred_knn\nsubmission_knn = submission_knn.merge(test_data, on=[\"PassengerId\"], how=\"left\")\nsubmission_knn.loc[submission_knn['Ticket_sv']==1.0, \"Survived\"] = 1\nsubmission_knn.loc[submission_knn['Ticket_sv']==0.0, \"Survived\"] = 0\ndel submission_knn['Ticket_sv']\nsubmission_knn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル融合（モデル１とモデル２の平均を取る）\n#### \"Survived\"=0.5の人の判別はそれぞれ個人の情報を抽出し、個別に判定しようとしたが、審査でHand_Labelingの対象となる可能性があるため、0.5→1とした場合と0.5→0とした場合の両方のサブミッションを提出し、スコアの良い方（0.5→0）を最終モデルとした。"},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_pred = submission_rf.merge(submission_knn, on=[\"PassengerId\"], how=\"left\")\nsum_pred[\"Survived_x\"] = sum_pred[\"Survived_x\"].astype(int)\nsum_pred[\"Survived_sum\"] = (sum_pred[\"Survived_x\"] + sum_pred[\"Survived_y\"])/2\nsum_pred[\"Survived_sum\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(418):\n    if sum_pred[\"Survived_sum\"][i]<=0.5:\n        sum_pred[\"Survived_sum\"][i]=0\n    else:\n        sum_pred[\"Survived_sum\"][i]=1\nsum_pred[\"Survived_sum\"] = sum_pred[\"Survived_sum\"].astype(int)        \nsum_pred[\"Survived_sum\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# モデル提出"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": pre_test[\"PassengerId\"],\n        \"Survived\":pre_test[\"Survived_sum\"]\n    })\n\nsubmission.to_csv('submission.Titanic_Best1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}